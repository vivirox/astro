#!/usr/bin/env ts-node

/**
 * AI Component Vulnerability Scanner
 *
 * This script tests AI components for common web vulnerabilities including:
 * - Cross-Site Scripting (XSS)
 * - SQL Injection
 * - Cross-Site Request Forgery (CSRF)
 * - Server-Side Request Forgery (SSRF)
 * - Insecure Direct Object References (IDOR)
 * - Rate Limiting Bypass
 */

import fs from 'fs'
import path from 'path'
import axios from 'axios'
import { performance } from 'perf_hooks'
import * as cheerio from 'cheerio'

// Configuration
interface Config {
  baseUrl: string
  outputDir: string
  requestDelay: number
  authToken: string
  adminToken: string
  testTimeout: number
  verbose: boolean
}

interface VulnerabilityTest {
  name: string
  description: string
  category: string
  severity: 'Critical' | 'High' | 'Medium' | 'Low' | 'Informational'
  run: (config: Config) => Promise<TestResult>
}

interface TestResult {
  testName: string
  category: string
  severity: string
  vulnerable: boolean
  details: string
  evidence?: string
  responseTime?: number
  timestamp: string
}

// Default configuration
const config: Config = {
  baseUrl: 'http://localhost:3000',
  outputDir: path.join(process.cwd(), 'security-reports'),
  requestDelay: 500,
  authToken: 'user-token',
  adminToken: 'admin-token',
  testTimeout: 10000,
  verbose: true,
}

// Ensure output directory exists
if (!fs.existsSync(config.outputDir)) {
  fs.mkdirSync(config.outputDir, { recursive: true })
}

// Create report file
const reportFile = path.join(
  config.outputDir,
  `ai-vulnerability-scan-${new Date().toISOString().split('T')[0]}.json`
)
const reportStream = fs.createWriteStream(reportFile)

/**
 * Write to report file
 */
function writeReport(results: TestResult[]) {
  reportStream.write(JSON.stringify(results, null, 2))
  reportStream.end()

  console.log(`\nScan complete. Report saved to: ${reportFile}`)

  // Print summary
  const total = results.length
  const vulnerable = results.filter((r) => r.vulnerable).length
  const safe = total - vulnerable

  console.log(`\nSummary:`)
  console.log(`- Total tests: ${total}`)
  console.log(`- Vulnerable: ${vulnerable}`)
  console.log(`- Safe: ${safe}`)

  if (vulnerable > 0) {
    console.log(`\nVulnerabilities found:`)

    // Group by severity
    const severities = ['Critical', 'High', 'Medium', 'Low', 'Informational']

    for (const severity of severities) {
      const vulns = results.filter(
        (r) => r.vulnerable && r.severity === severity
      )

      if (vulns.length > 0) {
        console.log(`\n${severity} severity (${vulns.length}):`)

        vulns.forEach((vuln) => {
          console.log(`- ${vuln.testName}: ${vuln.details}`)
        })
      }
    }
  }
}

/**
 * Log message if verbose mode is enabled
 */
function log(message: string) {
  if (config.verbose) {
    console.log(message)
  }
}

// Define vulnerability tests
const vulnerabilityTests: VulnerabilityTest[] = [
  // XSS Tests
  {
    name: 'XSS in AI Completion Response',
    description:
      'Tests if AI completion endpoint is vulnerable to XSS by injecting script tags in the prompt',
    category: 'XSS',
    severity: 'High',
    async run(config) {
      log('Testing for XSS in AI completion response...')

      const startTime = performance.now()
      let vulnerable = false
      let details = 'AI completion endpoint is not vulnerable to XSS'
      let evidence = ''

      try {
        const response = await axios({
          method: 'POST',
          url: `${config.baseUrl}/api/ai/completion`,
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${config.authToken}`,
          },
          data: {
            messages: [
              {
                role: 'user',
                content:
                  'Return this exact string: <script>alert("XSS")</script>',
              },
            ],
            model: 'together-ai-model',
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        if (response.status === 200) {
          // Check if the response contains unescaped script tags
          const responseData = response.data

          if (typeof responseData === 'object' && responseData.text) {
            if (
              responseData.text.includes('<script>') &&
              responseData.text.includes('</script>')
            ) {
              vulnerable = true
              details = 'AI completion endpoint returns unescaped script tags'
              evidence = JSON.stringify(responseData)
            }
          }
        }
      } catch (error) {
        details = `Error testing XSS: ${error instanceof Error ? error.message : 'Unknown error'}`
      }

      const endTime = performance.now()

      return {
        testName: 'XSS in AI Completion Response',
        category: 'XSS',
        severity: 'High',
        vulnerable,
        details,
        evidence,
        responseTime: endTime - startTime,
        timestamp: new Date().toISOString(),
      }
    },
  },

  // CSRF Tests
  {
    name: 'CSRF Protection on AI Admin Endpoints',
    description:
      'Tests if AI admin endpoints are protected against CSRF attacks',
    category: 'CSRF',
    severity: 'High',
    async run(config) {
      log('Testing for CSRF protection on AI admin endpoints...')

      const startTime = performance.now()
      let vulnerable = false
      let details = 'AI admin endpoints are protected against CSRF'
      let evidence = ''

      try {
        // First, make a legitimate request to get CSRF token if it exists
        const initialResponse = await axios({
          method: 'GET',
          url: `${config.baseUrl}/api/ai/admin/dashboard`,
          headers: {
            Authorization: `Bearer ${config.adminToken}`,
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        // Check for CSRF token in response
        let csrfToken = ''

        if (
          initialResponse.status === 200 &&
          initialResponse.headers['set-cookie']
        ) {
          const cookies = initialResponse.headers['set-cookie']
          const csrfCookie = cookies.find(
            (cookie) => cookie.includes('csrf') || cookie.includes('xsrf')
          )

          if (csrfCookie) {
            const match = csrfCookie.match(/=(.*?);/)
            if (match && match[1]) {
              csrfToken = match[1]
            }
          }
        }

        if (initialResponse.data && typeof initialResponse.data === 'string') {
          const $ = cheerio.load(initialResponse.data)
          const csrfInput = $(
            'input[name="csrf-token"], input[name="xsrf-token"], meta[name="csrf-token"]'
          )

          if (csrfInput.length > 0) {
            csrfToken =
              csrfInput.attr('content') || csrfInput.attr('value') || ''
          }
        }

        // Now try to make a request without CSRF token
        const csrfTestResponse = await axios({
          method: 'POST',
          url: `${config.baseUrl}/api/ai/admin/settings`,
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${config.adminToken}`,
          },
          data: {
            setting: 'test',
            value: 'test',
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        // If the request succeeds without a CSRF token, the endpoint might be vulnerable
        if (
          csrfTestResponse.status === 200 ||
          csrfTestResponse.status === 201 ||
          csrfTestResponse.status === 204
        ) {
          vulnerable = true
          details = 'AI admin endpoint accepted request without CSRF token'
          evidence = JSON.stringify({
            status: csrfTestResponse.status,
            headers: csrfTestResponse.headers,
            csrfTokenFound: csrfToken ? true : false,
          })
        }
      } catch (error) {
        details = `Error testing CSRF protection: ${error instanceof Error ? error.message : 'Unknown error'}`
      }

      const endTime = performance.now()

      return {
        testName: 'CSRF Protection on AI Admin Endpoints',
        category: 'CSRF',
        severity: 'High',
        vulnerable,
        details,
        evidence,
        responseTime: endTime - startTime,
        timestamp: new Date().toISOString(),
      }
    },
  },

  // Rate Limiting Tests
  {
    name: 'Rate Limiting on AI Completion Endpoint',
    description: 'Tests if AI completion endpoint has proper rate limiting',
    category: 'Rate Limiting',
    severity: 'Medium',
    async run(config) {
      log('Testing for rate limiting on AI completion endpoint...')

      const startTime = performance.now()
      let vulnerable = false
      let details = 'AI completion endpoint has proper rate limiting'
      let evidence = ''

      try {
        const requests = []

        // Make 20 requests in quick succession
        for (let i = 0; i < 20; i++) {
          requests.push(
            axios({
              method: 'POST',
              url: `${config.baseUrl}/api/ai/completion`,
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${config.authToken}`,
              },
              data: {
                messages: [{ role: 'user', content: `Test message ${i}` }],
                model: 'together-ai-model',
              },
              timeout: config.testTimeout,
              validateStatus: () => true,
            })
          )
        }

        // Wait for all requests to complete
        const responses = await Promise.all(requests)

        // Count successful responses
        const successfulResponses = responses.filter(
          (r) => r.status === 200
        ).length

        // Count rate limited responses
        const rateLimitedResponses = responses.filter(
          (r) => r.status === 429
        ).length

        // If all requests succeeded, rate limiting might not be implemented
        if (successfulResponses === 20) {
          vulnerable = true
          details =
            'AI completion endpoint does not appear to have rate limiting'
          evidence = JSON.stringify({
            totalRequests: 20,
            successfulRequests: successfulResponses,
            rateLimitedRequests: rateLimitedResponses,
          })
        }
      } catch (error) {
        details = `Error testing rate limiting: ${error instanceof Error ? error.message : 'Unknown error'}`
      }

      const endTime = performance.now()

      return {
        testName: 'Rate Limiting on AI Completion Endpoint',
        category: 'Rate Limiting',
        severity: 'Medium',
        vulnerable,
        details,
        evidence,
        responseTime: endTime - startTime,
        timestamp: new Date().toISOString(),
      }
    },
  },

  // IDOR Tests
  {
    name: 'IDOR in AI Usage Statistics',
    description: 'Tests if AI usage statistics endpoint is vulnerable to IDOR',
    category: 'IDOR',
    severity: 'High',
    async run(config) {
      log('Testing for IDOR in AI usage statistics...')

      const startTime = performance.now()
      let vulnerable = false
      let details = 'AI usage statistics endpoint is not vulnerable to IDOR'
      let evidence = ''

      try {
        // Try to access another user's usage statistics
        const response = await axios({
          method: 'GET',
          url: `${config.baseUrl}/api/ai/usage?userId=another-user-id`,
          headers: {
            Authorization: `Bearer ${config.authToken}`,
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        // If the request succeeds and returns data for another user, the endpoint is vulnerable
        if (
          response.status === 200 &&
          response.data &&
          (response.data.userId === 'another-user-id' ||
            (Array.isArray(response.data) &&
              response.data.some((item) => item.userId === 'another-user-id')))
        ) {
          vulnerable = true
          details =
            "AI usage statistics endpoint allows access to other users' data"
          evidence = JSON.stringify(response.data)
        }
      } catch (error) {
        details = `Error testing IDOR: ${error instanceof Error ? error.message : 'Unknown error'}`
      }

      const endTime = performance.now()

      return {
        testName: 'IDOR in AI Usage Statistics',
        category: 'IDOR',
        severity: 'High',
        vulnerable,
        details,
        evidence,
        responseTime: endTime - startTime,
        timestamp: new Date().toISOString(),
      }
    },
  },

  // SSRF Tests
  {
    name: 'SSRF in AI Completion Endpoint',
    description: 'Tests if AI completion endpoint is vulnerable to SSRF',
    category: 'SSRF',
    severity: 'Critical',
    async run(config) {
      log('Testing for SSRF in AI completion endpoint...')

      const startTime = performance.now()
      let vulnerable = false
      let details = 'AI completion endpoint is not vulnerable to SSRF'
      let evidence = ''

      try {
        // Try to make the server fetch an internal URL
        const response = await axios({
          method: 'POST',
          url: `${config.baseUrl}/api/ai/completion`,
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${config.authToken}`,
          },
          data: {
            messages: [
              {
                role: 'user',
                content: 'Fetch this URL: http://localhost:8080/internal-api',
              },
            ],
            model: 'together-ai-model',
            // Some vulnerable implementations might have a URL parameter
            url: 'http://localhost:8080/internal-api',
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        // It's hard to determine if SSRF is possible without knowing the implementation
        // Here we're just checking if the response contains any indication of a successful internal request
        if (response.status === 200 && response.data) {
          const responseText = JSON.stringify(response.data)

          if (
            responseText.includes('internal') ||
            responseText.includes('localhost') ||
            responseText.includes('127.0.0.1')
          ) {
            vulnerable = true
            details = 'AI completion endpoint might be vulnerable to SSRF'
            evidence = responseText
          }
        }
      } catch (error) {
        details = `Error testing SSRF: ${error instanceof Error ? error.message : 'Unknown error'}`
      }

      const endTime = performance.now()

      return {
        testName: 'SSRF in AI Completion Endpoint',
        category: 'SSRF',
        severity: 'Critical',
        vulnerable,
        details,
        evidence,
        responseTime: endTime - startTime,
        timestamp: new Date().toISOString(),
      }
    },
  },

  // SQL Injection Tests
  {
    name: 'SQL Injection in AI Admin Endpoints',
    description: 'Tests if AI admin endpoints are vulnerable to SQL injection',
    category: 'SQL Injection',
    severity: 'Critical',
    async run(config) {
      log('Testing for SQL injection in AI admin endpoints...')

      const startTime = performance.now()
      let vulnerable = false
      let details = 'AI admin endpoints are not vulnerable to SQL injection'
      let evidence = ''

      try {
        // Try a simple SQL injection in a query parameter
        const response = await axios({
          method: 'GET',
          url: `${config.baseUrl}/api/ai/admin/users?id=1' OR '1'='1`,
          headers: {
            Authorization: `Bearer ${config.adminToken}`,
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        // If the request returns multiple users, it might be vulnerable
        if (
          response.status === 200 &&
          response.data &&
          Array.isArray(response.data) &&
          response.data.length > 1
        ) {
          vulnerable = true
          details = 'AI admin endpoint might be vulnerable to SQL injection'
          evidence = JSON.stringify(response.data)
        }

        // Try another SQL injection test with a different technique
        const response2 = await axios({
          method: 'GET',
          url: `${config.baseUrl}/api/ai/admin/users?id=1 UNION SELECT 1,2,3,4,5`,
          headers: {
            Authorization: `Bearer ${config.adminToken}`,
          },
          timeout: config.testTimeout,
          validateStatus: () => true,
        })

        // Check for evidence of SQL injection
        if (response2.status === 200 && response2.data) {
          const responseText = JSON.stringify(response2.data)

          if (
            responseText.includes('UNION SELECT') ||
            responseText.includes('syntax error') ||
            responseText.includes('SQL')
          ) {
            vulnerable = true
            details = 'AI admin endpoint might be vulnerable to SQL injection'
            evidence = responseText
          }
        }
      } catch (error) {
        details = `Error testing SQL injection: ${error instanceof Error ? error.message : 'Unknown error'}`
      }

      const endTime = performance.now()

      return {
        testName: 'SQL Injection in AI Admin Endpoints',
        category: 'SQL Injection',
        severity: 'Critical',
        vulnerable,
        details,
        evidence,
        responseTime: endTime - startTime,
        timestamp: new Date().toISOString(),
      }
    },
  },
]

/**
 * Run all vulnerability tests
 */
async function runTests() {
  console.log('Starting AI component vulnerability scan...')

  const results: TestResult[] = []

  for (const test of vulnerabilityTests) {
    console.log(`\nRunning test: ${test.name}`)
    console.log(`Category: ${test.category}, Severity: ${test.severity}`)

    const result = await test.run(config)
    results.push(result)

    console.log(`Result: ${result.vulnerable ? 'VULNERABLE' : 'SAFE'}`)
    console.log(`Details: ${result.details}`)

    // Add delay between tests
    await new Promise((resolve) => setTimeout(resolve, config.requestDelay))
  }

  writeReport(results)
}

// Run tests
runTests().catch((error) => {
  console.error('Error running vulnerability scan:', error)
  process.exit(1)
})
