\doxysection{src/lib/ai/providers Directory Reference}
\hypertarget{dir_bf9f6ea2b0d861aa068a88f8ba01b82f}{}\label{dir_bf9f6ea2b0d861aa068a88f8ba01b82f}\index{src/lib/ai/providers Directory Reference@{src/lib/ai/providers Directory Reference}}


\doxysubsection{Detailed Description}
This directory contains the implementation of the Modal provider for the AI service. The Modal provider allows you to use custom LLM models deployed on Modal\textquotesingle{}s serverless infrastructure.\hypertarget{README.md_autotoc_md37418}{}\doxysubsection{\texorpdfstring{Overview}{Overview}}\label{README.md_autotoc_md37418}
Modal is a cloud platform that provides convenient, on-\/demand access to serverless cloud compute from Python scripts on your local computer. It\textquotesingle{}s particularly useful for running custom LLM models without depending on external LLM APIs.\hypertarget{README.md_autotoc_md37419}{}\doxysubsection{\texorpdfstring{Setup}{Setup}}\label{README.md_autotoc_md37419}

\begin{DoxyEnumerate}
\item Install Modal CLI\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{pip\ install\ modal}

\end{DoxyCode}

\item Set up Modal credentials\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{modal\ setup}

\end{DoxyCode}

\item Deploy a custom model using the provided script\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{python\ src/scripts/deploy-\/modal-\/model.py\ -\/-\/model\ llama3-\/8b}

\end{DoxyCode}

\item Configure environment variables\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{MODAL\_API\_URL=https://your-\/app-\/name-\/-\/serve.modal.run}
\DoxyCodeLine{MODAL\_API\_KEY=your-\/modal-\/api-\/key}

\end{DoxyCode}

\end{DoxyEnumerate}\hypertarget{README.md_autotoc_md37420}{}\doxysubsection{\texorpdfstring{Usage}{Usage}}\label{README.md_autotoc_md37420}

\begin{DoxyCode}{0}
\DoxyCodeLine{import\ \{\ createAIService\ \}\ from\ '../lib/ai'}
\DoxyCodeLine{}
\DoxyCodeLine{//\ Create\ AI\ service\ with\ Modal\ provider}
\DoxyCodeLine{const\ aiService\ =\ createAIService(\{}
\DoxyCodeLine{\ \ modal:\ \{}
\DoxyCodeLine{\ \ \ \ baseUrl:\ process.env.MODAL\_API\_URL,}
\DoxyCodeLine{\ \ \ \ apiKey:\ process.env.MODAL\_API\_KEY,}
\DoxyCodeLine{\ \ \},}
\DoxyCodeLine{\ \ defaultProvider:\ 'modal'}
\DoxyCodeLine{\})}
\DoxyCodeLine{}
\DoxyCodeLine{//\ Use\ the\ service}
\DoxyCodeLine{const\ response\ =\ await\ aiService.createChatCompletion(}
\DoxyCodeLine{\ \ [}
\DoxyCodeLine{\ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ role:\ 'system',}
\DoxyCodeLine{\ \ \ \ \ \ content:\ 'You\ are\ a\ helpful\ assistant.'}
\DoxyCodeLine{\ \ \ \ \},}
\DoxyCodeLine{\ \ \ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ role:\ 'user',}
\DoxyCodeLine{\ \ \ \ \ \ content:\ 'Hello,\ how\ are\ you?'}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\ \ ],}
\DoxyCodeLine{\ \ \{}
\DoxyCodeLine{\ \ \ \ model:\ 'llama3-\/8b-\/modal',}
\DoxyCodeLine{\ \ \ \ temperature:\ 0.7,}
\DoxyCodeLine{\ \ \ \ maxTokens:\ 1000}
\DoxyCodeLine{\ \ \}}
\DoxyCodeLine{)}

\end{DoxyCode}
\hypertarget{README.md_autotoc_md37421}{}\doxysubsection{\texorpdfstring{Available Models}{Available Models}}\label{README.md_autotoc_md37421}
The following models are pre-\/configured in the registry\+:


\begin{DoxyItemize}
\item {\ttfamily llama3-\/8b-\/modal}\+: LLa\+MA 3 8B (Modal)
\item {\ttfamily llama3-\/70b-\/modal}\+: LLa\+MA 3 70B (Modal)
\item {\ttfamily mistral-\/7b-\/modal}\+: Mistral 7B (Modal)
\item {\ttfamily custom-\/model-\/modal}\+: Custom Model (Modal)
\end{DoxyItemize}\hypertarget{README.md_autotoc_md37422}{}\doxysubsection{\texorpdfstring{Deployment Script}{Deployment Script}}\label{README.md_autotoc_md37422}
The {\ttfamily deploy-\/modal-\/model.\+py} script in the {\ttfamily src/scripts} directory allows you to deploy custom models to Modal. It supports various models like LLa\+MA 3, Mistral, and others.


\begin{DoxyCode}{0}
\DoxyCodeLine{\#\ Deploy\ LLaMA\ 3\ 8B}
\DoxyCodeLine{python\ src/scripts/deploy-\/modal-\/model.py\ -\/-\/model\ llama3-\/8b}
\DoxyCodeLine{}
\DoxyCodeLine{\#\ Deploy\ Mistral\ 7B}
\DoxyCodeLine{python\ src/scripts/deploy-\/modal-\/model.py\ -\/-\/model\ mistral-\/7b}
\DoxyCodeLine{}
\DoxyCodeLine{\#\ Deploy\ a\ custom\ model}
\DoxyCodeLine{python\ src/scripts/deploy-\/modal-\/model.py\ -\/-\/model\ custom-\/model\ -\/-\/model-\/id\ meta-\/llama/Llama-\/3-\/8B-\/Instruct}

\end{DoxyCode}
\hypertarget{README.md_autotoc_md37423}{}\doxysubsection{\texorpdfstring{Testing}{Testing}}\label{README.md_autotoc_md37423}
You can test the Modal provider using the provided test page at {\ttfamily /admin/modal-\/test}. This page allows you to select a model and send a prompt to test the integration.\hypertarget{README.md_autotoc_md37424}{}\doxysubsection{\texorpdfstring{Implementation Details}{Implementation Details}}\label{README.md_autotoc_md37424}
The Modal provider implements the same interface as other providers like Open\+AI and Anthropic, making it easy to switch between providers. It supports both streaming and non-\/streaming responses, and handles errors gracefully.

The provider communicates with the Modal-\/deployed model using the Open\+AI-\/compatible API, which makes it compatible with the existing AI service infrastructure. 