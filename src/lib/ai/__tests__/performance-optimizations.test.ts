import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest'
import { createAdvancedOptimizedAIService } from '../performance-optimizations'
import type { AIService, AICompletionRequest } from '../models/ai-types'
import * as performanceTracker from '../performance-tracker'

// Mock the performance tracker
vi.mock('../performance-tracker', () => ({
  trackPerformance: vi.fn(),
}))

// Extended request type for tests that need additional properties
interface ExtendedAICompletionRequest extends AICompletionRequest {
  userId?: string
  sessionId?: string
}

describe('Advanced Performance Optimizations', () => {
  let mockService: AIService
  let completeMock: ReturnType<typeof vi.fn>

  beforeEach(() => {
    // Reset mocks
    vi.resetAllMocks()

    // Create a mock AI service
    completeMock = vi.fn()
    mockService = {
      createChatCompletion: completeMock,
      getModelInfo: vi.fn(),
      createStreamingChatCompletion: vi.fn(),
      createChatCompletionWithTracking: vi.fn(),
      generateCompletion: vi.fn(),
      dispose: vi.fn(),
    }

    // Setup default mock response
    completeMock.mockResolvedValue({
      id: 'test-id',
      created: Date.now(),
      model: 'test-model',
      content: 'Test response',
      choices: [{ message: { role: 'assistant', content: 'Test response' } }],
      usage: {
        promptTokens: 10,
        completionTokens: 5,
        totalTokens: 15,
      },
    })
  })

  afterEach(() => {
    vi.resetAllMocks()
  })

  it('should pass through requests to the underlying service', async () => {
    const optimizedService = createAdvancedOptimizedAIService(mockService)

    const request: AICompletionRequest = {
      model: 'test-model',
      messages: [{ role: 'user', content: 'Hello', name: 'user' }],
    }

    await optimizedService.createChatCompletion(request.messages, {
      model: request.model,
    })

    expect(completeMock).toHaveBeenCalledWith(
      request.messages,
      expect.objectContaining({
        model: request.model,
      })
    )
  })

  it('should track performance metrics', async () => {
    const optimizedService = createAdvancedOptimizedAIService(mockService)

    const request: ExtendedAICompletionRequest = {
      model: 'test-model',
      messages: [{ role: 'user', content: 'Hello', name: 'user' }],
      userId: 'test-user',
    }

    await optimizedService.createChatCompletion(request.messages, {
      model: request.model,
      userId: request.userId,
    })

    expect(performanceTracker.trackPerformance).toHaveBeenCalledTimes(1)
    expect(performanceTracker.trackPerformance).toHaveBeenCalledWith(
      expect.objectContaining({
        model: 'test-model',
        success: true,
        cached: false,
        optimized: false,
        user_id: 'test-user',
      })
    )
  })

  it('should cache responses and return cached results', async () => {
    const optimizedService = createAdvancedOptimizedAIService(mockService, {
      caching: {
        enabled: true,
        ttl: 60, // 1 minute
      },
    })

    const request: AICompletionRequest = {
      model: 'test-model',
      messages: [{ role: 'user', content: 'Hello', name: 'user' }],
    }

    // First request should hit the service
    await optimizedService.createChatCompletion(request.messages, {
      model: request.model,
    })
    expect(completeMock).toHaveBeenCalledTimes(1)

    // Second identical request should use cache
    await optimizedService.createChatCompletion(request.messages, {
      model: request.model,
    })
    expect(completeMock).toHaveBeenCalledTimes(1) // Still only called once

    // Different request should hit the service again
    await optimizedService.createChatCompletion(
      [{ role: 'user', content: 'Different message', name: 'user' }],
      { model: request.model }
    )
    expect(completeMock).toHaveBeenCalledTimes(2)
  })

  it('should optimize tokens for long conversations', async () => {
    const optimizedService = createAdvancedOptimizedAIService(mockService, {
      tokenOptimization: {
        enabled: true,
        maxContextTokens: 100, // Very low for testing
      },
    })

    // Create a long conversation
    const longRequest: AICompletionRequest = {
      model: 'test-model',
      messages: [
        {
          role: 'system',
          content: 'You are a helpful assistant.',
          name: 'system',
        },
        {
          role: 'user',
          content:
            'Message 1 that is quite long and would exceed our token limit if we included all messages.',
          name: 'user',
        },
        {
          role: 'assistant',
          content:
            'Response 1 that is also quite verbose and would contribute to exceeding our token limit.',
          name: 'assistant',
        },
        {
          role: 'user',
          content:
            'Message 2 with additional context that would definitely push us over the limit.',
          name: 'user',
        },
        {
          role: 'assistant',
          content: 'Response 2 continuing the conversation with more tokens.',
          name: 'assistant',
        },
        {
          role: 'user',
          content: 'Final message that should be included since it is recent.',
          name: 'user',
        },
      ],
    }

    await optimizedService.createChatCompletion(longRequest.messages, {
      model: longRequest.model,
    })

    // The service should have been called with a subset of messages
    expect(completeMock).toHaveBeenCalledWith(
      expect.arrayContaining([
        expect.objectContaining({ role: 'system' }),
        // Should include the most recent messages but not all
      ]),
      expect.objectContaining({
        model: 'test-model',
      })
    )

    // The optimized request should have fewer messages than the original
    const calledWith = completeMock.mock.calls[0][0]
    expect(calledWith.length).toBeLessThan(longRequest.messages.length)

    // System message should always be included
    expect(calledWith[0].role).toBe('system')

    // Last message should always be included
    const lastMessageIndex = calledWith.length - 1
    expect(calledWith[lastMessageIndex].content).toBe(
      'Final message that should be included since it is recent.'
    )
  })

  it('should handle rate limiting', async () => {
    const optimizedService = createAdvancedOptimizedAIService(mockService, {
      enableRateLimit: true,
      maxRequestsPerMinute: 2,
    })

    const request: ExtendedAICompletionRequest = {
      model: 'test-model',
      messages: [{ role: 'user', content: 'Hello', name: 'user' }],
      userId: 'rate-limited-user',
    }

    // First request should succeed
    await optimizedService.createChatCompletion(request.messages, {
      model: request.model,
      userId: request.userId,
    })
    expect(completeMock).toHaveBeenCalledTimes(1)

    // Second request should succeed
    await optimizedService.createChatCompletion(request.messages, {
      model: request.model,
      userId: request.userId,
    })
    expect(completeMock).toHaveBeenCalledTimes(2)

    // Third request should be rate limited
    await expect(
      optimizedService.createChatCompletion(request.messages, {
        model: request.model,
        userId: request.userId,
      })
    ).rejects.toThrow('Rate limit exceeded')
    expect(completeMock).toHaveBeenCalledTimes(2) // Still only called twice

    // Should track the failed request
    expect(performanceTracker.trackPerformance).toHaveBeenCalledTimes(3)
    expect(performanceTracker.trackPerformance).toHaveBeenLastCalledWith(
      expect.objectContaining({
        success: false,
        error_code: 'RATE_LIMITED',
      })
    )
  })

  it('should try fallback models when primary model fails', async () => {
    // Make the primary model fail
    completeMock.mockImplementation((options) => {
      if (options?.model === 'primary-model') {
        return Promise.reject(new Error('Primary model error'))
      }
      return Promise.resolve({
        id: 'fallback-id',
        created: Date.now(),
        model: options?.model,
        content: 'Fallback response',
        choices: [
          { message: { role: 'assistant', content: 'Fallback response' } },
        ],
        usage: {
          promptTokens: 10,
          completionTokens: 5,
          totalTokens: 15,
        },
      })
    })

    const optimizedService = createAdvancedOptimizedAIService(mockService, {
      enableFallback: true,
      fallbackModels: ['fallback-model-1', 'fallback-model-2'],
    })

    const request: AICompletionRequest = {
      model: 'primary-model',
      messages: [{ role: 'user', content: 'Hello', name: 'user' }],
    }

    const response = await optimizedService.createChatCompletion(
      request.messages,
      { model: request.model }
    )

    // Should have tried the fallback model
    expect(completeMock).toHaveBeenCalledWith(
      expect.objectContaining({
        model: 'fallback-model-1',
      })
    )

    // Should return the fallback response
    expect(response?.model).toBe('fallback-model-1')
    expect(response?.content).toBe('Fallback response')

    // Should track both the failed request and successful fallback
    expect(performanceTracker.trackPerformance).toHaveBeenCalledTimes(2)
  })
})
